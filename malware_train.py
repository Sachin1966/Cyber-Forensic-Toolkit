import pandas as pd
import numpy as np
import joblib
import os
import gc
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer

# ---------------------------------------
# Configuration
# ---------------------------------------
MODELS_DIR = "./models"
os.makedirs(MODELS_DIR, exist_ok=True)
MAX_TOTAL_ROWS = 200000  # Cap total rows to prevent OOM
CHUNK_SIZE = 50000       # Read large files in chunks

# ---------------------------------------
# 1. Global malware label map
# ---------------------------------------
LABEL_MAP = {
    # Benign/Normal
    "benign": 0, "legit": 0, "legitimate": 0, "normal": 0, "good": 0, "0": 0, "safe": 0,
    
    # Malware/Attack
    "malware": 1, "attack": 1, "malicious": 1, "bad": 1, "1": 1, "virus": 1,
    "trojan": 1, "ransomware": 1, "spyware": 1, "adware": 1,
    
    # Network/IoT specific
    "exploits": 1, "fuzzers": 1, "analysis": 1, "reconnaissance": 1,
    "backdoor": 1, "ddos": 1, "dos": 1, "generic": 1, "shellcode": 1,
    "worms": 1, "ftp-patator": 1, "ssh-patator": 1, "infiltration": 1,
    "botnet": 1, "bruteforce": 1, "portscan": 1
}

def clean_malware_labels(series):
    """Normalize labels to 0 (Benign) and 1 (Malicious)."""
    # Convert to string, lower case, strip whitespace
    s = series.astype(str).str.lower().str.strip()
    # Map known labels
    mapped = s.map(LABEL_MAP)
    # Fill unknown with NaN (to be dropped later) or assume malicious if suspicious? 
    # Better to drop ambiguous data.
    return mapped

def safe_read_csv(filepath):
    """Try multiple engines and encodings to read a CSV."""
    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            # Try auto-detect separator first
            return pd.read_csv(filepath, sep=None, engine='python', encoding=encoding, on_bad_lines='skip', low_memory=False)
        except Exception:
            try:
                # Fallback to standard comma
                return pd.read_csv(filepath, sep=',', encoding=encoding, on_bad_lines='skip', low_memory=False)
            except Exception:
                continue
    return None

def process_file(filepath, target_rows):
    """Load, clean, and sample a single file."""
    try:
        df = safe_read_csv(filepath)
        if df is None or df.empty:
            print(f"‚ö† Skipping {os.path.basename(filepath)}: Could not read file.")
            return None

        print(f"‚úî Loaded {os.path.basename(filepath)} ‚Üí {df.shape}")

        # 1. Detect Label Column
        label_candidates = [c for c in df.columns if any(k in c.lower() for k in
            ["label", "malware", "attack", "class", "category", "target", "type"])]
        
        if not label_candidates:
            print(f"  ‚ö† Skipping {os.path.basename(filepath)}: No label column found.")
            return None
        
        label_col = label_candidates[0]
        
        # 2. Clean Labels
        df["label"] = clean_malware_labels(df[label_col])
        df = df.dropna(subset=["label"]) # Drop rows with unknown labels
        
        if df.empty:
            print(f"  ‚ö† Skipping {os.path.basename(filepath)}: No valid labels found.")
            return None

        df["label"] = df["label"].astype(np.int8)
        
        # 3. Aggressive Feature Cleaning
        # Drop the original label column if it's not 'label'
        if label_col != "label":
            df = df.drop(columns=[label_col])
            
        # Convert all other columns to numeric, coercing errors to NaN
        # This handles mixed types, strings in numeric cols, etc.
        for col in df.columns:
            if col != "label":
                df[col] = pd.to_numeric(df[col], errors="coerce")
        
        # Drop columns that are completely empty/NaN
        df.dropna(axis=1, how='all', inplace=True)
        
        # Keep only numeric columns + label
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df = df[numeric_cols]
        
        # 4. Downcast to save memory
        fcols = df.select_dtypes('float').columns
        icols = df.select_dtypes('integer').columns
        
        if len(fcols) > 0:
            df[fcols] = df[fcols].astype(np.float32)
        if len(icols) > 0:
            df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')

        # 5. Sample if too large
        if len(df) > target_rows:
            df = df.sample(target_rows, random_state=42)
            print(f"  ‚Üò Sampled to {len(df)} rows")
            
        return df

    except Exception as e:
        print(f"‚ö† Error processing {os.path.basename(filepath)}: {e}")
        return None

def train_malware(malware_files):
    print("\n============== ROBUST MALWARE MODEL TRAINING ==============")
    
    dfs = []
    # Calculate target rows per file to stay within memory limits
    rows_per_file = max(5000, int(MAX_TOTAL_ROWS / max(1, len(malware_files))))

    for f in malware_files:
        df = process_file(f, rows_per_file)
        if df is not None:
            dfs.append(df)
            gc.collect() # Force garbage collection

    if not dfs:
        print("‚ùå No valid data found to train on.")
        return

    print("\nCombinining datasets...")
    # Align columns (fill missing with 0 for features not present in some files)
    full_df = pd.concat(dfs, ignore_index=True, sort=False)
    del dfs
    gc.collect()
    
    # Fill NaNs (from coercion or missing cols) with 0
    full_df.fillna(0, inplace=True)
    
    print(f"üìå Final Training Data: {full_df.shape}")
    
    if "label" not in full_df.columns:
        print("‚ùå Error: Label column missing in final dataset.")
        return

    # Split X, y
    X = full_df.drop("label", axis=1)
    y = full_df["label"]
    
    # Final check for infinite values
    X.replace([np.inf, -np.inf], 0, inplace=True)

    # Save feature names for inference alignment
    feature_names = X.columns.tolist()
    joblib.dump(feature_names, f"{MODELS_DIR}/malware_features.pkl")
    print(f"üìÅ Saved feature names: {len(feature_names)} features")

    # Scale
    print("Scaling features...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Calculate Accuracy (on training set for now, or split if needed)
    # For speed/simplicity in this script, we'll use training score or a simple split
    # Better to do a quick validation split
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    import json
    from datetime import datetime

    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    print("üî• Training RandomForest model...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=20,
        n_jobs=-1,
        random_state=42,
        class_weight='balanced'
    )
    model.fit(X_train, y_train)
    
    val_accuracy = model.score(X_val, y_val) * 100
    print(f"‚úÖ Validation Accuracy: {val_accuracy:.2f}%")

    # Save
    joblib.dump(model, f"{MODELS_DIR}/malware_model.pkl")
    joblib.dump(scaler, f"{MODELS_DIR}/malware_scaler.pkl")
    
    # Update Metadata
    metadata_path = os.path.join(os.path.dirname(__file__), 'forensic-ai-hub', 'backend', 'models', 'metadata.json')
    try:
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
        else:
            metadata = {}
            
        if 'malware' not in metadata:
            metadata['malware'] = {}
            
        metadata['malware']['accuracy'] = round(val_accuracy, 1)
        metadata['malware']['last_trained'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        metadata['malware']['status'] = 'Ready'
        
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"Updated metadata at {metadata_path}")
    except Exception as e:
        print(f"‚ùå Failed to update metadata: {e}")

    print("\nüéâ Malware Model Training Complete!")
    print(f"üìÅ Saved to: {os.path.abspath(MODELS_DIR)}")
    print("=====================================================\n")

if __name__ == "__main__":
    dataset_dir = os.path.join(os.path.dirname(__file__), 'dataset')
    
    malware_files = []
    if os.path.exists(dataset_dir):
        for root, dirs, files in os.walk(dataset_dir):
            for file in files:
                if file.endswith(".csv"):
                    malware_files.append(os.path.join(root, file))
    
    if malware_files:
        print(f"Found {len(malware_files)} CSV files.")
        train_malware(malware_files)
    else:
        print(f"No CSV files found in {dataset_dir}")
